{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35afe405",
   "metadata": {},
   "source": [
    "√âtape 1 : Charger les donn√©es\n",
    "\n",
    "Je v√©rifiecoh√©rence entre les colonnes de train.csv et test.csv, car cela peut effectivement poser probl√®me lors de la pr√©diction:\n",
    "- train.csv a une colonne Survived (car c‚Äôest ce que tu veux apprendre √† pr√©dire).\n",
    "- test.csv n‚Äôa pas cette colonne (car c‚Äôest ce que tu veux pr√©dire).\n",
    "\n",
    "Donc l'absence de Survived dans test.csv est parfaitement normale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0fbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Donn√©es d'entra√Ænement (train.csv) :\n",
      "---------------------------\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "---------------------------\n",
      "Forme du train.csv : (891, 12)\n",
      "---------------------------\n",
      " Donn√©es d'entra√Ænement (test.csv) :\n",
      "---------------------------\n",
      "   PassengerId  Pclass                                          Name     Sex  \\\n",
      "0          892       3                              Kelly, Mr. James    male   \n",
      "1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
      "2          894       2                     Myles, Mr. Thomas Francis    male   \n",
      "3          895       3                              Wirz, Mr. Albert    male   \n",
      "4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
      "\n",
      "    Age  SibSp  Parch   Ticket     Fare Cabin Embarked Survived  \n",
      "0  34.5      0      0   330911   7.8292   NaN        Q     None  \n",
      "1  47.0      1      0   363272   7.0000   NaN        S     None  \n",
      "2  62.0      0      0   240276   9.6875   NaN        Q     None  \n",
      "3  27.0      0      0   315154   8.6625   NaN        S     None  \n",
      "4  22.0      1      1  3101298  12.2875   NaN        S     None  \n",
      "Forme du train.csv : (418, 12)\n",
      "---------------------------\n",
      "     PassengerId Survived  Pclass  \\\n",
      "0              1        0       3   \n",
      "1              2        1       1   \n",
      "2              3        1       3   \n",
      "3              4        1       1   \n",
      "4              5        0       3   \n",
      "..           ...      ...     ...   \n",
      "413         1305     None       3   \n",
      "414         1306     None       1   \n",
      "415         1307     None       3   \n",
      "416         1308     None       3   \n",
      "417         1309     None       3   \n",
      "\n",
      "                                                  Name     Sex   Age  SibSp  \\\n",
      "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                             Allen, Mr. William Henry    male  35.0      0   \n",
      "..                                                 ...     ...   ...    ...   \n",
      "413                                 Spector, Mr. Woolf    male   NaN      0   \n",
      "414                       Oliva y Ocana, Dona. Fermina  female  39.0      0   \n",
      "415                       Saether, Mr. Simon Sivertsen    male  38.5      0   \n",
      "416                                Ware, Mr. Frederick    male   NaN      0   \n",
      "417                           Peter, Master. Michael J    male   NaN      1   \n",
      "\n",
      "     Parch              Ticket      Fare Cabin Embarked  \n",
      "0        0           A/5 21171    7.2500   NaN        S  \n",
      "1        0            PC 17599   71.2833   C85        C  \n",
      "2        0    STON/O2. 3101282    7.9250   NaN        S  \n",
      "3        0              113803   53.1000  C123        S  \n",
      "4        0              373450    8.0500   NaN        S  \n",
      "..     ...                 ...       ...   ...      ...  \n",
      "413      0           A.5. 3236    8.0500   NaN        S  \n",
      "414      0            PC 17758  108.9000  C105        C  \n",
      "415      0  SOTON/O.Q. 3101262    7.2500   NaN        S  \n",
      "416      0              359309    8.0500   NaN        S  \n",
      "417      1                2668   22.3583   NaN        C  \n",
      "\n",
      "[1309 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "# √âtape 1 : Charger les donn√©es\n",
    "import pandas as pd\n",
    "\n",
    "# Chargement des donn√©es\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "\n",
    "# Aper√ßu des donn√©es\n",
    "print(\" Donn√©es d'entra√Ænement (train.csv) :\")\n",
    "print(train_df.head())\n",
    "print(f\"Forme du train.csv : {train_df.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44691949",
   "metadata": {},
   "source": [
    "√âtape 2 ‚Äî Inspection des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "38f1653c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Nombre de lignes dupliqu√©es dans train.csv : 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\" D√©tecter les doublons \"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#  D√©tecter les lignes dupliqu√©es\n",
    "doublons = train_df.duplicated()\n",
    "\n",
    "#  Compter le nombre total de doublons\n",
    "nombre_doublons = doublons.sum()\n",
    "\n",
    "print(f\" Nombre de lignes dupliqu√©es dans train.csv : {nombre_doublons}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "f375a8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Valeurs manquantes dans train.csv :\n",
      "          Valeurs manquantes  % du total\n",
      "Cabin                    687   77.104377\n",
      "Age                      177   19.865320\n",
      "Embarked                   2    0.224467\n"
     ]
    }
   ],
   "source": [
    "\"\"\" D√©tecter les valeurs manquanntes \"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Calcul du nombre et du pourcentage de valeurs manquantes\n",
    "missing_count = train_df.isnull().sum()\n",
    "missing_percent = 100 * missing_count / len(train_df)\n",
    "\n",
    "# Rassembler dans un DataFrame lisible\n",
    "missing_data = pd.DataFrame({\n",
    "    'Valeurs manquantes': missing_count,\n",
    "    '% du total': missing_percent\n",
    "})\n",
    "\n",
    "# Filtrer les colonnes avec au moins 1 valeur manquante\n",
    "missing_data = missing_data[missing_data['Valeurs manquantes'] > 0]\n",
    "\n",
    "# Trier par pourcentage d√©croissant\n",
    "missing_data = missing_data.sort_values(by='% du total', ascending=False)\n",
    "\n",
    "# Afficher les r√©sultats\n",
    "print(\"üìä Valeurs manquantes dans train.csv :\")\n",
    "print(missing_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0719e1fd",
   "metadata": {},
   "source": [
    "√âtape 3 - Traitement des donn√©es manquantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172a9447",
   "metadata": {},
   "source": [
    "√âtape 3-1 ‚Äî Extraire le Deck : \n",
    "\n",
    "Chaque cabine a une valeur comme \"C85\" ou \"E46\" ou \"C23 C25 C27\".\n",
    "On veut extraire uniquement la premi√®re lettre, qui indique le pont du navire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "fdaeef0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      NaN\n",
      "1        C\n",
      "2      NaN\n",
      "3        C\n",
      "4      NaN\n",
      "      ... \n",
      "886    NaN\n",
      "887      B\n",
      "888    NaN\n",
      "889      C\n",
      "890    NaN\n",
      "Name: Deck, Length: 891, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Extraire la premi√®re lettre (deck)\n",
    "train_df[\"Deck\"] = train_df[\"Cabin\"].str.extract(r'([A-Za-z])')\n",
    "print(train_df[\"Deck\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bceaf13",
   "metadata": {},
   "source": [
    "√âtape 3-2 ‚Äî Cr√©er Has_Cabin (0 ou 1)\n",
    "On veut savoir si une personne avait une cabine renseign√©e ou non.\n",
    "Cela peut refl√©ter un statut social √©lev√© ou un acc√®s plus facile aux canots de sauvetage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "757d0448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Has_Cabin                                               Name\n",
      "0            0                            Braund, Mr. Owen Harris\n",
      "1            1  Cumings, Mrs. John Bradley (Florence Briggs Th...\n",
      "2            0                             Heikkinen, Miss. Laina\n",
      "3            1       Futrelle, Mrs. Jacques Heath (Lily May Peel)\n",
      "4            0                           Allen, Mr. William Henry\n",
      "..         ...                                                ...\n",
      "886          0                              Montvila, Rev. Juozas\n",
      "887          1                       Graham, Miss. Margaret Edith\n",
      "888          0           Johnston, Miss. Catherine Helen \"Carrie\"\n",
      "889          1                              Behr, Mr. Karl Howell\n",
      "890          0                                Dooley, Mr. Patrick\n",
      "\n",
      "[891 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Cr√©er une colonne binaire : 1 si la cabine est connue, 0 sinon\n",
    "train_df[\"Has_Cabin\"] = train_df[\"Cabin\"].notnull().astype(int)\n",
    "print(train_df[[\"Has_Cabin\",\"Name\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "08a9d893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deck\n",
      "NaN    687\n",
      "C       59\n",
      "B       47\n",
      "D       33\n",
      "E       32\n",
      "A       15\n",
      "F       13\n",
      "G        4\n",
      "T        1\n",
      "Name: count, dtype: int64\n",
      "Has_Cabin\n",
      "0    687\n",
      "1    204\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# V√©rification\n",
    "print(train_df[\"Deck\"].value_counts(dropna=False))  # Y compris les NaN\n",
    "print(train_df[\"Has_Cabin\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f5c165",
   "metadata": {},
   "source": [
    "√âtape 3-3 ‚Äî V√©rifier les valeurs manquantes sur la colonne Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "a22df86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(177)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"Age\"].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3330371a",
   "metadata": {},
   "source": [
    "√âtape 3-4 ‚Äî Imputation avanc√©e Age par combinaison Title, Sex, Pclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "84f6b45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title\n",
      "Mr              517\n",
      "Miss            182\n",
      "Mrs             125\n",
      "Master           40\n",
      "Dr                7\n",
      "Rev               6\n",
      "Col               2\n",
      "Mlle              2\n",
      "Major             2\n",
      "Ms                1\n",
      "Mme               1\n",
      "Don               1\n",
      "Lady              1\n",
      "Sir               1\n",
      "Capt              1\n",
      "the Countess      1\n",
      "Jonkheer          1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Extraire les titres depuis Name\n",
    "train_df[\"Title\"] = train_df[\"Name\"].str.extract(r',\\s*([^\\.]+)\\.')\n",
    "print(train_df[\"Title\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "1f9cf2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title\n",
      "Mr        517\n",
      "Miss      185\n",
      "Mrs       126\n",
      "Master     40\n",
      "Rare       23\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#  Nettoyer et regrouper les titres rares\n",
    "title_mapping = {\n",
    "    \"Mlle\": \"Miss\", \"Ms\": \"Miss\", \"Mme\": \"Mrs\",\n",
    "    \"Lady\": \"Rare\", \"Countess\": \"Rare\", \"Capt\": \"Rare\",\n",
    "    \"Col\": \"Rare\", \"Don\": \"Rare\", \"Dr\": \"Rare\", \"Major\": \"Rare\",\n",
    "    \"Rev\": \"Rare\", \"Sir\": \"Rare\", \"Jonkheer\": \"Rare\", \"Dona\": \"Rare\", \"the Countess\": \"Rare\", \n",
    "}\n",
    "train_df[\"Title\"] = train_df[\"Title\"].replace(title_mapping)\n",
    "\n",
    "print(train_df[\"Title\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d4bcbf",
   "metadata": {},
   "source": [
    "√âtape 3-4 ‚Äî Calculer l'√¢ge moyen pour chaque combinaison Title + Sex + Pclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "c703e7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title   Sex     Pclass\n",
      "Master  male    1          5.306667\n",
      "                2          2.258889\n",
      "                3          5.350833\n",
      "Miss    female  1         29.744681\n",
      "                2         22.560606\n",
      "                3         16.123188\n",
      "Mr      male    1         41.580460\n",
      "                2         32.768293\n",
      "                3         28.724891\n",
      "Mrs     female  1         40.400000\n",
      "                2         33.682927\n",
      "                3         33.515152\n",
      "Rare    female  1         43.333333\n",
      "        male    1         48.727273\n",
      "                2         42.000000\n",
      "Name: Age, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# On cr√©e un tableau √† 3 dimensions :\n",
    "# Calcul des moyennes d'√¢ge\n",
    "age_means = train_df.groupby(['Title', 'Sex', 'Pclass'])['Age'].mean()\n",
    "print(age_means)\n",
    "# age_means contient maintenant les valeurs moyennes d‚Äô√¢ge pour chaque combinaison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "9207f7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction d‚Äôimputation selon ces 3 crit√®res\n",
    "def impute_age_advanced(row):\n",
    "    if pd.isnull(row['Age']):\n",
    "        return age_means.loc[row['Title'], row['Sex'], row['Pclass']]\n",
    "    else:\n",
    "        return row['Age']\n",
    "\n",
    "train_df[\"Age\"] = train_df.apply(impute_age_advanced, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "048b1d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# V√©rification\n",
    "print(train_df[\"Age\"].isnull().sum())  # Doit afficher 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "c9d16fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporter les donn√©es avec implentamentio de l'Age\n",
    "train_df.to_csv(\"train_implen_age.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c786a4",
   "metadata": {},
   "source": [
    "√âtape 4 ‚Äî Encoder les variables cat√©gorielles\n",
    "Transformer les colonnes comme Sex, Embarked, Title, Deck en nombres pour qu‚Äôelles soient compatibles avec le mod√®le.\n",
    "On va faire dans cet ordre :\n",
    "\n",
    "Sex ‚Üí label encoding (0/1)\n",
    "\n",
    "Embarked ‚Üí one-hot encoding (S, C, Q)\n",
    "\n",
    "Title ‚Üí one-hot encoding (Mr, Miss, etc.)\n",
    "\n",
    "Deck ‚Üí one-hot encoding (A, B, ..., NaN)\n",
    "\n",
    "Supprimer les colonnes inutiles (Name, Ticket, Cabin, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "0c6b9e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3526/2383733863.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_df[\"Embarked\"].fillna(\"S\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# 1. Sex\n",
    "train_df[\"Sex\"] = train_df[\"Sex\"].map({\"male\": 0, \"female\": 1})\n",
    "\n",
    "# 2. Embarked\n",
    "train_df[\"Embarked\"].fillna(\"S\", inplace=True)\n",
    "embarked_dummies = pd.get_dummies(train_df[\"Embarked\"], prefix=\"Embarked\")\n",
    "train_df = pd.concat([train_df, embarked_dummies], axis=1)\n",
    "train_df.drop(\"Embarked\", axis=1, inplace=True)\n",
    "\n",
    "# 3. Title\n",
    "title_dummies = pd.get_dummies(train_df[\"Title\"], prefix=\"Title\")\n",
    "train_df = pd.concat([train_df, title_dummies], axis=1)\n",
    "train_df.drop(\"Title\", axis=1, inplace=True)\n",
    "\n",
    "# 4. Deck\n",
    "deck_dummies = pd.get_dummies(train_df[\"Deck\"], prefix=\"Deck\")\n",
    "train_df = pd.concat([train_df, deck_dummies], axis=1)\n",
    "train_df.drop(\"Deck\", axis=1, inplace=True)\n",
    "\n",
    "# 5. Supprimer les colonnes non n√©cessaires\n",
    "train_df.drop(columns=[\"Name\", \"Ticket\", \"PassengerId\", \"Cabin\"], inplace=True, errors=\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "6137a6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived          int64\n",
      "Pclass            int64\n",
      "Sex               int64\n",
      "Age             float64\n",
      "SibSp             int64\n",
      "Parch             int64\n",
      "Fare            float64\n",
      "Has_Cabin         int64\n",
      "Embarked_C         bool\n",
      "Embarked_Q         bool\n",
      "Embarked_S         bool\n",
      "Title_Master       bool\n",
      "Title_Miss         bool\n",
      "Title_Mr           bool\n",
      "Title_Mrs          bool\n",
      "Title_Rare         bool\n",
      "Deck_A             bool\n",
      "Deck_B             bool\n",
      "Deck_C             bool\n",
      "Deck_D             bool\n",
      "Deck_E             bool\n",
      "Deck_F             bool\n",
      "Deck_G             bool\n",
      "Deck_T             bool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "1a9852fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir les colonnes bool ‚Üí int (Gradient Boosting les traite mieux comme 0/1)\n",
    "\n",
    "bool_cols = train_df.select_dtypes('bool').columns\n",
    "train_df[bool_cols] = train_df[bool_cols].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "c484ee4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived          int64\n",
      "Pclass            int64\n",
      "Sex               int64\n",
      "Age             float64\n",
      "SibSp             int64\n",
      "Parch             int64\n",
      "Fare            float64\n",
      "Has_Cabin         int64\n",
      "Embarked_C        int64\n",
      "Embarked_Q        int64\n",
      "Embarked_S        int64\n",
      "Title_Master      int64\n",
      "Title_Miss        int64\n",
      "Title_Mr          int64\n",
      "Title_Mrs         int64\n",
      "Title_Rare        int64\n",
      "Deck_A            int64\n",
      "Deck_B            int64\n",
      "Deck_C            int64\n",
      "Deck_D            int64\n",
      "Deck_E            int64\n",
      "Deck_F            int64\n",
      "Deck_G            int64\n",
      "Deck_T            int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# V√©rification\n",
    "print(train_df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebe6b4f",
   "metadata": {},
   "source": [
    "### √âtape 5 ‚Äî Entra√Ænement du mod√®le avec Gradient Boosting avec les donn√©es d'entrainement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "15cfd462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8101\n",
      "Cross-validation mean accuracy: 0.8238\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Chargement des donn√©es (tu peux adapter le chemin si besoin)\n",
    "\n",
    "# S√©paration des features et de la cible\n",
    "X = train_df.drop(\"Survived\", axis=1)\n",
    "y = train_df['Survived']\n",
    "\n",
    "\n",
    "# Train / validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialisation et entra√Ænement du mod√®le\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=100,       # nombre d'arbres\n",
    "    learning_rate=0.1,      # taux d'apprentissage\n",
    "    max_depth=3,            # profondeur max des arbres\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# √âvaluation sur validation\n",
    "y_pred = gb_model.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# (Optionnel) Validation crois√©e\n",
    "scores = cross_val_score(gb_model, X, y, cv=5)\n",
    "print(f\"Cross-validation mean accuracy: {scores.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950606f2",
   "metadata": {},
   "source": [
    "Etape 5-1 - Optimisation des hyperparam√®tres d'entrainement Gradien Boosting avec GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "639b0e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   0.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   0.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   0.4s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   0.4s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   0.4s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   0.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   0.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   0.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   0.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   0.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   0.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   0.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   0.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   0.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   0.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   0.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   0.6s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   0.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   0.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   0.6s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   0.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   0.6s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   0.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   0.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   0.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   0.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   0.6s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   0.6s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   0.6s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   0.6s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   0.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   0.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   0.6s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   0.6s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   0.4s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   0.4s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   0.6s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   0.6s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   0.4s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   0.4s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   0.4s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   0.8s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   0.8s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   0.8s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   0.8s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   0.8s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   0.9s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   0.8s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   0.9s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   0.4s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   0.5s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   0.9s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   0.9s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   0.5s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   0.5s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   0.4s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   0.9s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   1.0s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   0.9s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   1.0s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   0.8s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   0.8s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   0.8s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   0.8s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   0.3s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   0.3s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   0.8s\n",
      "[CV] END learning_rate=0.05, max_depth=4, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   0.8s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   0.3s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   0.3s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   0.3s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   0.3s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   0.3s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   0.3s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   0.3s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   0.3s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   0.6s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   0.6s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   0.6s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   0.7s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   0.8s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   0.7s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   0.8s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   0.7s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   0.3s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   0.3s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   0.7s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   0.7s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   0.4s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   0.4s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   0.4s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   0.3s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   0.3s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   0.3s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   0.7s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   0.7s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   0.7s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   0.7s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   0.7s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   0.7s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   0.8s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   0.8s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   0.4s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   0.4s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   0.7s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   0.7s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   0.4s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   0.4s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   0.4s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   0.8s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   0.8s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   0.8s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   0.8s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   0.8s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   0.9s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   0.9s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   0.9s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   0.4s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   0.4s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   0.8s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   0.8s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   0.4s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   0.4s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   0.4s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   0.8s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   0.9s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   0.9s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   1.0s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   0.9s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   0.9s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   0.9s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   0.9s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   0.7s\n",
      "[CV] END learning_rate=0.1, max_depth=4, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   0.8s\n",
      " Meilleurs param√®tres : {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.8}\n",
      " Meilleure pr√©cision moyenne : 0.8440\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 1. D√©finir le mod√®le de base\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# 2. Grille d'hyperparam√®tres √† tester\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [3, 4],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# 3. Configuration de GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=gb,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,                  # validation crois√©e √† 5 plis\n",
    "    scoring='accuracy',    # on cherche la meilleure pr√©cision\n",
    "    n_jobs=-1,             # utilise tous les c≈ìurs CPU\n",
    "    verbose=2              # affiche la progression\n",
    ")\n",
    "\n",
    "# 4. Lancer la recherche\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# 5. R√©sultats\n",
    "print(\" Meilleurs param√®tres :\", grid_search.best_params_)\n",
    "print(f\" Meilleure pr√©cision moyenne : {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# 6. R√©cup√©rer le meilleur mod√®le\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da693c0",
   "metadata": {},
   "source": [
    "√âtape 5-2 ‚Äî Entra√Ænement du mod√®le avec Gradient Boosting et optimisation des hyperparameters\n",
    "\n",
    " Meilleurs param√®tres : {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.8}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "78ad671b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8268\n",
      "Cross-validation mean accuracy: 0.8440\n"
     ]
    }
   ],
   "source": [
    "# Initialisation et entra√Ænement du mod√®le avec les nouveaux hyperparameter\n",
    "gb_model_optimised = GradientBoostingClassifier(\n",
    "    n_estimators=200,       # nombre d'arbres\n",
    "    learning_rate=0.1,      # taux d'apprentissage\n",
    "    max_depth=3,            # profondeur max des arbres\n",
    "    random_state=42,\n",
    "    min_samples_split= 5,\n",
    "    subsample=0.8,\n",
    ")\n",
    "\n",
    "gb_model_optimised.fit(X_train, y_train)\n",
    "\n",
    "# √âvaluation sur validation\n",
    "y_pred = gb_model_optimised.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# (Optionnel) Validation crois√©e\n",
    "scores = cross_val_score(gb_model_optimised, X, y, cv=5)\n",
    "print(f\"Cross-validation mean accuracy: {scores.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8f8d1d",
   "metadata": {},
   "source": [
    "## √âtape 6 ‚Äî Entra√Ænement du mod√®le avec Gradient Boosting avec les donn√©es de test\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
